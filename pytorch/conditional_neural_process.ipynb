{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-31T11:42:59.963122Z",
     "start_time": "2020-07-31T11:42:55.970766Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.distributions import Normal, Independent\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import collections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-31T11:43:00.010738Z",
     "start_time": "2020-07-31T11:42:59.987457Z"
    }
   },
   "outputs": [],
   "source": [
    "class GPCurvesReader(object):\n",
    "    \"\"\"Generates curves using a Gaussian Process (GP).\n",
    "\n",
    "    Supports vector inputs (x) and vector outputs (y). Kernel is\n",
    "    mean-squared exponential, using the x-value l2 coordinate distance scaled by\n",
    "    some factor chosen randomly in a range. Outputs are independent gaussian\n",
    "    processes.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "               batch_size,\n",
    "               max_num_context,\n",
    "               x_size=1,\n",
    "               y_size=1,\n",
    "               l1_scale=0.4,\n",
    "               sigma_scale=1.0,\n",
    "               testing=False):\n",
    "        \"\"\"Creates a regression dataset of functions sampled from a GP.\n",
    "\n",
    "        Args:\n",
    "          batch_size: An integer.\n",
    "          max_num_context: The max number of observations in the context.\n",
    "          x_size: Integer >= 1 for length of \"x values\" vector.\n",
    "          y_size: Integer >= 1 for length of \"y values\" vector.\n",
    "          l1_scale: Float; typical scale for kernel distance function.\n",
    "          sigma_scale: Float; typical scale for variance.\n",
    "          testing: Boolean that indicates whether we are testing. If so there are\n",
    "              more targets for visualization.\n",
    "        \"\"\"\n",
    "        self._batch_size = batch_size\n",
    "        self._max_num_context = max_num_context\n",
    "        self._x_size = x_size\n",
    "        self._y_size = y_size\n",
    "        self._l1_scale = l1_scale\n",
    "        self._sigma_scale = sigma_scale\n",
    "        self._testing = testing\n",
    "\n",
    "    def _gaussian_kernel(self, xdata, l1, sigma_f, sigma_noise=2e-2):\n",
    "        \"\"\"Applies the Gaussian kernel to generate curve data.\n",
    "\n",
    "        Args:\n",
    "          xdata: Tensor with shape `[batch_size, num_total_points, x_size]` with\n",
    "              the values of the x-axis data.\n",
    "          l1: Tensor with shape `[batch_size, y_size, x_size]`, the scale\n",
    "              parameter of the Gaussian kernel.\n",
    "          sigma_f: Float tensor with shape `[batch_size, y_size]`; the magnitude\n",
    "              of the std.\n",
    "          sigma_noise: Float, std of the noise that we add for stability.\n",
    "\n",
    "        Returns:\n",
    "          The kernel, a float tensor with shape\n",
    "          `[batch_size, y_size, num_total_points, num_total_points]`.\n",
    "        \"\"\"\n",
    "        num_total_points = xdata.shape[1]\n",
    "\n",
    "        # Expand and take the difference\n",
    "        xdata1 = torch.unsqueeze(xdata, 1)  # [B, 1, num_total_points, x_size]\n",
    "        xdata2 = torch.unsqueeze(xdata, 2)  # [B, num_total_points, 1, x_size]\n",
    "        diff = xdata1 - xdata2  # [B, num_total_points, num_total_points, x_size]\n",
    "\n",
    "        # [B, y_size, num_total_points, num_total_points, x_size]\n",
    "        norm = torch.square(diff[:, None, :, :, :] / l1[:, :, None, None, :])\n",
    "\n",
    "        norm = torch.sum(norm, -1)  # [B, data_size, num_total_points, num_total_points]\n",
    "\n",
    "        # [B, y_size, num_total_points, num_total_points]\n",
    "        kernel = torch.square(sigma_f)[:, :, None, None] * torch.exp(-0.5 * norm)\n",
    "\n",
    "        # Add some noise to the diagonal to make the cholesky work.\n",
    "        kernel += (sigma_noise**2) * torch.eye(num_total_points)\n",
    "\n",
    "        return kernel\n",
    "\n",
    "    def generate_curves(self):\n",
    "        \"\"\"Builds the op delivering the data.\n",
    "\n",
    "        Generated functions are `float32` with x values between -2 and 2.\n",
    "\n",
    "        Returns:\n",
    "          A `CNPRegressionDescription` namedtuple.\n",
    "        \"\"\"\n",
    "        num_context = torch.randint(size=[], low=3, high=self._max_num_context)\n",
    "\n",
    "        # If we are testing we want to have more targets and have them evenly\n",
    "        # distributed in order to plot the function.\n",
    "        if self._testing:\n",
    "            num_target = 400\n",
    "            num_total_points = num_target\n",
    "            x_values = torch.unsqueeze(torch.arange(-2.,2.,1./100), 0).repeat([self._batch_size, 1])\n",
    "            x_values = torch.unsqueeze(x_values, axis=-1)\n",
    "        # During training the number of target points and their x-positions are\n",
    "        # selected at random\n",
    "        else:\n",
    "            num_target = torch.randint(size=[], low=2, high=self._max_num_context)\n",
    "            num_total_points = num_context + num_target\n",
    "            x_values = torch.Tensor(self._batch_size, num_total_points, self._x_size).uniform_(-2, 2)\n",
    "\n",
    "        # Set kernel parameters\n",
    "        l1 = torch.ones([self._batch_size, self._y_size, self._x_size])*self._l1_scale\n",
    "        sigma_f = torch.ones([self._batch_size, self._y_size]) * self._sigma_scale\n",
    "\n",
    "        # Pass the x_values through the Gaussian kernel\n",
    "        # [batch_size, y_size, num_total_points, num_total_points]\n",
    "        kernel = self._gaussian_kernel(x_values, l1, sigma_f)\n",
    "\n",
    "        # Calculate Cholesky, using double precision for better stability:\n",
    "        cholesky = torch.cholesky(kernel.double()).float()\n",
    "\n",
    "        # Sample a curve\n",
    "        # [batch_size, y_size, num_total_points, 1]\n",
    "        y_values = torch.matmul(cholesky,torch.normal(0.0, 1.0, [self._batch_size, self._y_size, num_total_points, 1]))\n",
    "\n",
    "        # [batch_size, num_total_points, y_size]\n",
    "        y_values = torch.squeeze(y_values, 3).permute(0, 2, 1)\n",
    "\n",
    "        if self._testing:\n",
    "          # Select the targets\n",
    "            target_x = x_values\n",
    "            target_y = y_values\n",
    "\n",
    "            # Select the observations\n",
    "            #### MAYBE WRONG\n",
    "            idx = torch.randperm(num_target)\n",
    "            context_x = x_values[:, idx[:num_context], :]\n",
    "            context_y = y_values[:, idx[:num_context], :]\n",
    "\n",
    "        else:\n",
    "            # Select the targets which will consist of the context points as well as\n",
    "            # some new target points\n",
    "            target_x = x_values[:, :num_target + num_context, :]\n",
    "            target_y = y_values[:, :num_target + num_context, :]\n",
    "\n",
    "            # Select the observations\n",
    "            context_x = x_values[:, :num_context, :]\n",
    "            context_y = y_values[:, :num_context, :]\n",
    "\n",
    "\n",
    "        return context_x, context_y, target_x, target_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-29T14:10:08.812534Z",
     "start_time": "2020-07-29T14:10:08.787114Z"
    }
   },
   "source": [
    "## Encoder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-31T11:43:00.040996Z",
     "start_time": "2020-07-31T11:43:00.027623Z"
    }
   },
   "outputs": [],
   "source": [
    "class DeterministicEncoder(nn.Module):\n",
    "    \"\"\"The Encoder.\"\"\"\n",
    "    \n",
    "    def __init__(self, layer_sizes):\n",
    "        super(DeterministicEncoder, self).__init__()\n",
    "        self.Linears = nn.ModuleList([nn.Linear(s1,s2) for s1,s2 in zip(layer_sizes[:-1],layer_sizes[1:])])\n",
    "        \n",
    "\n",
    "    def forward(self, context_x, context_y):\n",
    "        # Concatenate x and y along the filter axes\n",
    "        x = torch.cat([context_x, context_y], dim=-1)\n",
    "        \n",
    "        batch_size, num_context_points, filter_size = x.shape\n",
    "        x = x.reshape(batch_size * num_context_points, -1)\n",
    "        \n",
    "        # Pass through MLP\n",
    "        for lay in self.Linears[:-1]:\n",
    "            x = F.relu(lay(x))\n",
    "        # Last layer without a ReLu\n",
    "        x = self.Linears[-1](x)\n",
    "        \n",
    "        x = x.reshape(batch_size, num_context_points, x.shape[-1])\n",
    "        return x.mean(axis=1)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-31T11:43:00.076245Z",
     "start_time": "2020-07-31T11:43:00.064372Z"
    }
   },
   "outputs": [],
   "source": [
    "class DeterministicDecoder(nn.Module):\n",
    "\n",
    "    def __init__(self, layer_sizes):\n",
    "        super(DeterministicDecoder, self).__init__()\n",
    "        self.Linears = nn.ModuleList([nn.Linear(s1,s2) for s1,s2 in zip(layer_sizes[:-1],layer_sizes[1:])])\n",
    "        \n",
    "    def forward(self, representation, target_x):\n",
    "        # Concatenate the representation and the target_x\n",
    "        num_total_points = target_x.shape[1]\n",
    "        representation = torch.unsqueeze(representation, 1).repeat([1, num_total_points, 1])\n",
    "        x = torch.cat([representation, target_x], dim=-1)\n",
    "        \n",
    "        batch_size, num_context_points, filter_size = x.shape\n",
    "        x = x.reshape(batch_size * num_context_points, -1)\n",
    "\n",
    "        # Pass through MLP\n",
    "        for lay in self.Linears[:-1]:\n",
    "            x = F.relu(lay(x))\n",
    "        # Last layer without a ReLu\n",
    "        x = self.Linears[-1](x)\n",
    "        x = x.reshape((batch_size, num_total_points, -1))\n",
    "        \n",
    "        # Get the mean an the variance\n",
    "        mu, log_sigma = torch.split(x, 1, dim=-1)\n",
    "        \n",
    "        # Bound the variance\n",
    "        sigma = 0.1 + 0.9 * F.softplus(log_sigma)\n",
    "        \n",
    "        # Get the distribution\n",
    "        dist = Independent(Normal(mu, sigma),1)\n",
    "        \n",
    "        return dist, mu, sigma\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-31T11:43:00.981711Z",
     "start_time": "2020-07-31T11:43:00.975346Z"
    }
   },
   "outputs": [],
   "source": [
    "class DeterministicModel(nn.Module):\n",
    "    \"\"\"The CNP model.\"\"\"\n",
    "\n",
    "    def __init__(self, encoder_layer_sizes, decoder_layer_sizes):\n",
    "        super(DeterministicModel, self).__init__()\n",
    "        self.encoder = DeterministicEncoder(encoder_layer_sizes)\n",
    "        self.decoder = DeterministicDecoder(decoder_layer_sizes)\n",
    "        \n",
    "    def forward(self, context_x, context_y, target_x):\n",
    "        # Pass query through the encoder and the decoder\n",
    "        representation = self.encoder(context_x, context_y)\n",
    "        dist, mu, sigma = self.decoder(representation, target_x)\n",
    "\n",
    "        return dist, mu, sigma  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-31T11:43:02.024567Z",
     "start_time": "2020-07-31T11:43:02.013013Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_functions(target_x, target_y, context_x, context_y, pred_y, var):\n",
    "    # Plot everything\n",
    "    plt.plot(target_x[0], pred_y[0], 'b', linewidth=2)\n",
    "    plt.plot(target_x[0], target_y[0], 'k:', linewidth=2)\n",
    "    plt.plot(context_x[0], context_y[0], 'ko', markersize=10)\n",
    "    plt.fill_between(\n",
    "        target_x[0, :, 0],\n",
    "        pred_y[0, :, 0] - var[0, :, 0],\n",
    "        pred_y[0, :, 0] + var[0, :, 0],\n",
    "        alpha=0.2,\n",
    "        facecolor='#65c9f7',\n",
    "        interpolate=True)\n",
    "\n",
    "    # Make the plot pretty\n",
    "    plt.yticks([-2, 0, 2], fontsize=16)\n",
    "    plt.xticks([-2, 0, 2], fontsize=16)\n",
    "    plt.ylim([-2, 2])\n",
    "    plt.grid('off')\n",
    "    ax = plt.gca()\n",
    "    ax.set_facecolor('white')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-31T11:43:27.631074Z",
     "start_time": "2020-07-31T11:43:27.626043Z"
    }
   },
   "outputs": [],
   "source": [
    "TRAINING_ITERATIONS = int(4e5)\n",
    "MAX_CONTEXT_POINTS = 15\n",
    "PLOT_AFTER = int(2e4)\n",
    "\n",
    "dataset_train = GPCurvesReader(batch_size=64, max_num_context=MAX_CONTEXT_POINTS)\n",
    "dataset_test = GPCurvesReader(batch_size=1, max_num_context=MAX_CONTEXT_POINTS, testing=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-31T11:44:24.156923Z",
     "start_time": "2020-07-31T11:44:24.147201Z"
    }
   },
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "encoder_output_sizes = [2, 128, 128, 128, 128]\n",
    "decoder_output_sizes = [129,128, 128, 2]\n",
    "\n",
    "model = DeterministicModel(encoder_output_sizes, decoder_output_sizes)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-31T11:56:02.292773Z",
     "start_time": "2020-07-31T11:46:33.338227Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for it in range(TRAINING_ITERATIONS):\n",
    "    train_context_x, train_context_y, train_target_x, train_target_y = dataset_train.generate_curves()\n",
    "    dist, _, _ = model(train_context_x, train_context_y, train_target_x)\n",
    "\n",
    "    log_p = dist.log_prob(train_target_y)\n",
    "    loss = -log_p.mean()\n",
    "    model.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Plot the predictions in `PLOT_AFTER` intervals\n",
    "    if it % PLOT_AFTER == 0:\n",
    "        # Get the predicted mean and variance at the target points for the testing set\n",
    "        test_context_x, test_context_y, test_target_x, test_target_y = dataset_test.generate_curves(\n",
    "        )\n",
    "        dist, mu, sigma = model(test_context_x, test_context_y, test_target_x)\n",
    "        log_p = dist.log_prob(test_target_y)\n",
    "        loss = -log_p.mean()\n",
    "\n",
    "        print('Iteration: {}, loss: {}'.format(it, loss.detach().numpy()))\n",
    "\n",
    "        # Plot the prediction and the context\n",
    "        plot_functions(test_target_x.detach().numpy(),\n",
    "                       test_target_y.detach().numpy(),\n",
    "                       test_context_x.detach().numpy(),\n",
    "                       test_context_y.detach().numpy(),\n",
    "                       mu.detach().numpy(),\n",
    "                       sigma.detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-30T13:39:38.107633Z",
     "start_time": "2020-07-30T13:39:38.103335Z"
    }
   },
   "outputs": [],
   "source": [
    "print(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepmind",
   "language": "python",
   "name": "deepmind"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
