{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-25T13:59:59.828924Z",
     "start_time": "2020-08-25T13:59:59.793597Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.distributions import Normal, kl_divergence\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import collections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-25T14:00:00.787465Z",
     "start_time": "2020-08-25T14:00:00.754232Z"
    }
   },
   "outputs": [],
   "source": [
    "class GPCurvesReader(object):\n",
    "    \"\"\"Generates curves using a Gaussian Process (GP).\n",
    "\n",
    "    Supports vector inputs (x) and vector outputs (y). Kernel is\n",
    "    mean-squared exponential, using the x-value l2 coordinate distance scaled by\n",
    "    some factor chosen randomly in a range. Outputs are independent gaussian\n",
    "    processes.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "               batch_size,\n",
    "               max_num_context,\n",
    "               x_size=1,\n",
    "               y_size=1,\n",
    "               l1_scale=0.4,\n",
    "               sigma_scale=1.0,\n",
    "               testing=False):\n",
    "        \"\"\"Creates a regression dataset of functions sampled from a GP.\n",
    "\n",
    "        Args:\n",
    "          batch_size: An integer.\n",
    "          max_num_context: The max number of observations in the context.\n",
    "          x_size: Integer >= 1 for length of \"x values\" vector.\n",
    "          y_size: Integer >= 1 for length of \"y values\" vector.\n",
    "          l1_scale: Float; typical scale for kernel distance function.\n",
    "          sigma_scale: Float; typical scale for variance.\n",
    "          testing: Boolean that indicates whether we are testing. If so there are\n",
    "              more targets for visualization.\n",
    "        \"\"\"\n",
    "        self._batch_size = batch_size\n",
    "        self._max_num_context = max_num_context\n",
    "        self._x_size = x_size\n",
    "        self._y_size = y_size\n",
    "        self._l1_scale = l1_scale\n",
    "        self._sigma_scale = sigma_scale\n",
    "        self._testing = testing\n",
    "\n",
    "    def _gaussian_kernel(self, xdata, l1, sigma_f, sigma_noise=2e-2):\n",
    "        \"\"\"Applies the Gaussian kernel to generate curve data.\n",
    "\n",
    "        Args:\n",
    "          xdata: Tensor with shape `[batch_size, num_total_points, x_size]` with\n",
    "              the values of the x-axis data.\n",
    "          l1: Tensor with shape `[batch_size, y_size, x_size]`, the scale\n",
    "              parameter of the Gaussian kernel.\n",
    "          sigma_f: Float tensor with shape `[batch_size, y_size]`; the magnitude\n",
    "              of the std.\n",
    "          sigma_noise: Float, std of the noise that we add for stability.\n",
    "\n",
    "        Returns:\n",
    "          The kernel, a float tensor with shape\n",
    "          `[batch_size, y_size, num_total_points, num_total_points]`.\n",
    "        \"\"\"\n",
    "        num_total_points = xdata.shape[1]\n",
    "\n",
    "        # Expand and take the difference\n",
    "        xdata1 = torch.unsqueeze(xdata, 1)  # [B, 1, num_total_points, x_size]\n",
    "        xdata2 = torch.unsqueeze(xdata, 2)  # [B, num_total_points, 1, x_size]\n",
    "        diff = xdata1 - xdata2  # [B, num_total_points, num_total_points, x_size]\n",
    "\n",
    "        # [B, y_size, num_total_points, num_total_points, x_size]\n",
    "        norm = torch.square(diff[:, None, :, :, :] / l1[:, :, None, None, :])\n",
    "\n",
    "        norm = torch.sum(norm, -1)  # [B, data_size, num_total_points, num_total_points]\n",
    "\n",
    "        # [B, y_size, num_total_points, num_total_points]\n",
    "        kernel = torch.square(sigma_f)[:, :, None, None] * torch.exp(-0.5 * norm)\n",
    "\n",
    "        # Add some noise to the diagonal to make the cholesky work.\n",
    "        kernel += (sigma_noise**2) * torch.eye(num_total_points)\n",
    "\n",
    "        return kernel\n",
    "\n",
    "    def generate_curves(self):\n",
    "        \"\"\"Builds the op delivering the data.\n",
    "\n",
    "        Generated functions are `float32` with x values between -2 and 2.\n",
    "\n",
    "        Returns:\n",
    "          A `CNPRegressionDescription` namedtuple.\n",
    "        \"\"\"\n",
    "        num_context = torch.randint(size=[], low=3, high=self._max_num_context)\n",
    "\n",
    "        # If we are testing we want to have more targets and have them evenly\n",
    "        # distributed in order to plot the function.\n",
    "        if self._testing:\n",
    "            num_target = 400\n",
    "            num_total_points = num_target\n",
    "            x_values = torch.unsqueeze(torch.arange(-2.,2.,1./100), 0).repeat([self._batch_size, 1])\n",
    "            x_values = torch.unsqueeze(x_values, axis=-1)\n",
    "        # During training the number of target points and their x-positions are\n",
    "        # selected at random\n",
    "        else:\n",
    "            num_target = torch.randint(size=[], low=2, high=self._max_num_context)\n",
    "            num_total_points = num_context + num_target\n",
    "            x_values = torch.Tensor(self._batch_size, num_total_points, self._x_size).uniform_(-2, 2)\n",
    "\n",
    "        # Set kernel parameters\n",
    "        l1 = torch.ones([self._batch_size, self._y_size, self._x_size])*self._l1_scale\n",
    "        sigma_f = torch.ones([self._batch_size, self._y_size]) * self._sigma_scale\n",
    "\n",
    "        # Pass the x_values through the Gaussian kernel\n",
    "        # [batch_size, y_size, num_total_points, num_total_points]\n",
    "        kernel = self._gaussian_kernel(x_values, l1, sigma_f)\n",
    "\n",
    "        # Calculate Cholesky, using double precision for better stability:\n",
    "        cholesky = torch.cholesky(kernel.double()).float()\n",
    "\n",
    "        # Sample a curve\n",
    "        # [batch_size, y_size, num_total_points, 1]\n",
    "        y_values = torch.matmul(cholesky,torch.normal(0.0, 1.0, [self._batch_size, self._y_size, num_total_points, 1]))\n",
    "\n",
    "        # [batch_size, num_total_points, y_size]\n",
    "        y_values = torch.squeeze(y_values, 3).permute(0, 2, 1)\n",
    "\n",
    "        if self._testing:\n",
    "          # Select the targets\n",
    "            target_x = x_values\n",
    "            target_y = y_values\n",
    "\n",
    "            # Select the observations\n",
    "            #### MAYBE WRONG\n",
    "            idx = torch.randperm(num_target)\n",
    "            context_x = x_values[:, idx[:num_context], :]\n",
    "            context_y = y_values[:, idx[:num_context], :]\n",
    "\n",
    "        else:\n",
    "            # Select the targets which will consist of the context points as well as\n",
    "            # some new target points\n",
    "            target_x = x_values[:, :num_target + num_context, :]\n",
    "            target_y = y_values[:, :num_target + num_context, :]\n",
    "\n",
    "            # Select the observations\n",
    "            context_x = x_values[:, :num_context, :]\n",
    "            context_y = y_values[:, :num_context, :]\n",
    "\n",
    "\n",
    "        return context_x, context_y, target_x, target_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-25T14:05:13.187113Z",
     "start_time": "2020-08-25T14:05:13.180798Z"
    }
   },
   "outputs": [],
   "source": [
    "def forward_pass(x, linears):\n",
    "    batch_size, num_context_points, filter_size = x.shape\n",
    "    x = x.reshape(batch_size * num_context_points, -1)\n",
    "    # Pass through MLP\n",
    "    for lay in self.Linears[:-1]:\n",
    "        x = F.relu(lay(x))\n",
    "    # Last layer without a ReLu\n",
    "    x = self.Linears[-1](x)\n",
    "    \n",
    "    x = x.reshape(batch_size, num_context_points, x.shape[-1])\n",
    "\n",
    "    return x\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder Deterministic Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-25T14:05:54.006765Z",
     "start_time": "2020-08-25T14:05:53.991152Z"
    }
   },
   "outputs": [],
   "source": [
    "class DeterministicEncoder(nn.Module):\n",
    "    \"\"\"The Deterministic Encoder.\"\"\"\n",
    "\n",
    "    def __init__(self, layer_sizes, attention):\n",
    "        \"\"\"(A)NP deterministic encoder.\n",
    "\n",
    "        Args:\n",
    "          output_sizes: An iterable containing the output sizes of the encoding MLP.\n",
    "          attention: The attention module.\n",
    "        \"\"\"\n",
    "        super(DeterministicEncoder, self).__init__()\n",
    "        self.Linears = nn.ModuleList([\n",
    "            nn.Linear(s1, s2) for s1, s2 in zip(layer_sizes[:-1], layer_sizes[1:])\n",
    "        ])\n",
    "        self.attention = attention\n",
    "\n",
    "    def forward(self, context_x, context_y, target_x):\n",
    "        \"\"\"Encodes the inputs into one representation.\n",
    "\n",
    "        Args:\n",
    "          context_x: Tensor of shape [B,observations,d_x]. For this 1D regression\n",
    "              task this corresponds to the x-values.\n",
    "          context_y: Tensor of shape [B,observations,d_y]. For this 1D regression\n",
    "              task this corresponds to the y-values.\n",
    "          target_x: Tensor of shape [B,target_observations,d_x]. \n",
    "              For this 1D regression task this corresponds to the x-values.\n",
    "\n",
    "        Returns:\n",
    "          The encoded representation. Tensor of shape [B,target_observations,d]\n",
    "        \"\"\"\n",
    "\n",
    "        # Concatenate x and y along the filter axes\n",
    "        x = torch.cat([context_x, context_y], axis=-1)\n",
    "        x = forward_pass(x)\n",
    "        x = self.attention(context_x, target_x, x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder : Latent Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-04T07:19:40.910765Z",
     "start_time": "2020-08-04T07:19:40.893462Z"
    }
   },
   "outputs": [],
   "source": [
    "class LatentEncoder(nn.Module):\n",
    "    \"\"\"The Latent Encoder.\"\"\"\n",
    "    def __init__(self, layer_sizes, num_latent):\n",
    "        \"\"\"(A)NP latent encoder.\n",
    "\n",
    "        Args:\n",
    "          output_sizes: An iterable containing the output sizes of the encoding MLP.\n",
    "          num_latents: The latent dimensionality.\n",
    "        \"\"\"\n",
    "        super(LatentEncoder, self).__init__()\n",
    "        self.Linears = nn.ModuleList([\n",
    "            nn.Linear(s1, s2)\n",
    "            for s1, s2 in zip(layer_sizes[:-2], layer_sizes[1:-1])\n",
    "        ])\n",
    "        last_layer_size = (layer_sizes[-1] + num_latent)/2\n",
    "        self.penultimate_layer = nn.Linear(layer_sizes[-2],last_layer_size)\n",
    "        self.mu_layer = nn.Linear(last_layer_size, num_latent)\n",
    "        self.std_layer = nn.Linear(last_layer_size, num_latent)\n",
    "\n",
    "    def forward(self, tx, ty):\n",
    "        \"\"\"Encodes the inputs into one representation.\n",
    "\n",
    "    Args:\n",
    "      x: Tensor of shape [B,observations,d_x]. For this 1D regression\n",
    "          task this corresponds to the x-values.\n",
    "      y: Tensor of shape [B,observations,d_y]. For this 1D regression\n",
    "          task this corresponds to the y-values.\n",
    "\n",
    "    Returns:\n",
    "      A normal distribution over tensors of shape [B, num_latents]\n",
    "    \"\"\"\n",
    "\n",
    "        # Concatenate x and y along the filter axes\n",
    "        x = torch.cat([tx, ty], axis=-1)\n",
    "        x = forward_pass(x, self.Linears)\n",
    "\n",
    "        # Aggregator: take the mean over all points\n",
    "        x = x.mean(axis=1)\n",
    "\n",
    "        # First apply intermediate relu layer\n",
    "        x = F.relu(self.penultimate_layer(x))\n",
    "            \n",
    "        # Then apply further linear layers to output latent mu and log sigma\n",
    "        mu = self.mu_layer(x)\n",
    "        log_sigma = self.mu_layer(x)\n",
    "\n",
    "        # Compute sigma\n",
    "        sigma = 0.1 + 0.9 * tf.sigmoid(log_sigma)\n",
    "\n",
    "        return Normal(loc=mu, scale=sigma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-04T07:19:41.846179Z",
     "start_time": "2020-08-04T07:19:41.836593Z"
    }
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \"\"\"The Decoder.\"\"\"\n",
    "    def __init__(self, layer_sizes):\n",
    "        \"\"\"(A)NP decoder.\n",
    "\n",
    "        Args:\n",
    "          output_sizes: An iterable containing the output sizes of the decoder MLP \n",
    "              as defined in `basic.Linear`.\n",
    "        \"\"\"\n",
    "        super(Decoder, self).__init__()\n",
    "        self.Linears = nn.ModuleList([\n",
    "            nn.Linear(s1, s2)\n",
    "            for s1, s2 in zip(layer_sizes[:-1], layer_sizes[1:])\n",
    "        ])\n",
    "\n",
    "    def forward(self, representation, target_x):\n",
    "        \"\"\"Decodes the individual targets.\n",
    "\n",
    "        Args:\n",
    "          representation: The representation of the context for target predictions. \n",
    "              Tensor of shape [B,target_observations,?].\n",
    "          target_x: The x locations for the target query.\n",
    "              Tensor of shape [B,target_observations,d_x].\n",
    "\n",
    "        Returns:\n",
    "          dist: A multivariate Gaussian over the target points. A distribution over\n",
    "              tensors of shape [B,target_observations,d_y].\n",
    "          mu: The mean of the multivariate Gaussian.\n",
    "              Tensor of shape [B,target_observations,d_x].\n",
    "          sigma: The standard deviation of the multivariate Gaussian.\n",
    "              Tensor of shape [B,target_observations,d_x].\n",
    "        \"\"\"\n",
    "        # concatenate target_x and representation\n",
    "        x = torch.cat([representation, target_x], axis=-1)\n",
    "\n",
    "        x = forward_pass(x, self.Linears)\n",
    "\n",
    "        # Get the mean an the variance\n",
    "        mu, log_sigma = torch.split(hidden, 1, dim=-1)\n",
    "\n",
    "        # Bound the variance\n",
    "        sigma = 0.1 + 0.9 * F.softplus(log_sigma)\n",
    "\n",
    "        # Get the distribution\n",
    "        dist = Independent(Normal(mu, sigma), 1)\n",
    "\n",
    "        return dist, mu, sigma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ANP Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-25T14:12:54.998522Z",
     "start_time": "2020-08-25T14:12:54.969923Z"
    }
   },
   "outputs": [],
   "source": [
    "class LatentModel(nn.Module):\n",
    "    \"\"\"The (A)NP model.\"\"\"\n",
    "    def __init__(self,\n",
    "                 latent_encoder_layer_sizes,\n",
    "                 num_latents,\n",
    "                 decoder_layer_sizes,\n",
    "                 use_deterministic_path=True,\n",
    "                 deterministic_encoder_layer_sizes=None,\n",
    "                 attention=None):\n",
    "        \"\"\"Initialises the model.\n",
    "\n",
    "        Args:\n",
    "          latent_encoder_output_sizes: An iterable containing the sizes of hidden \n",
    "              layers of the latent encoder.\n",
    "          num_latents: The latent dimensionality.\n",
    "          decoder_output_sizes: An iterable containing the sizes of hidden layers of\n",
    "              the decoder. The last element should correspond to d_y * 2\n",
    "              (it encodes both mean and variance concatenated)\n",
    "          use_deterministic_path: a boolean that indicates whether the deterministic\n",
    "              encoder is used or not.\n",
    "          deterministic_encoder_output_sizes: An iterable containing the sizes of \n",
    "              hidden layers of the deterministic encoder. The last one is the size \n",
    "              of the deterministic representation r.\n",
    "          attention: The attention module used in the deterministic encoder.\n",
    "              Only relevant when use_deterministic_path=True.\n",
    "        \"\"\"\n",
    "        self._latent_encoder = LatentEncoder(latent_encoder_output_sizes,num_latents)\n",
    "        self._decoder = Decoder(decoder_output_sizes)\n",
    "        self._use_deterministic_path = use_deterministic_path\n",
    "        if use_deterministic_path:\n",
    "            self._deterministic_encoder = DeterministicEncoder(deterministic_encoder_output_sizes, attention)\n",
    "\n",
    "    def forward(forward, context_x, context_y, target_x, target_y=None):\n",
    "        # Pass query through the encoder and the decoder\n",
    "        prior = self._latent_encoder(context_x, context_y)\n",
    "\n",
    "        # For training, when target_y is available, use targets for latent encoder.\n",
    "        # Note that targets contain contexts by design.\n",
    "        if target_y is None:\n",
    "            latent_rep = prior.sample()\n",
    "        # For testing, when target_y unavailable, use contexts for latent encoder.\n",
    "        else:\n",
    "            posterior = self._latent_encoder(target_x, target_y)\n",
    "            latent_rep = posterior.sample()\n",
    "            \n",
    "        latent_rep = latent_rep.unsqueeze(1).repeat([1, num_targets, 1])\n",
    "        \n",
    "        if self._use_deterministic_path:\n",
    "            deterministic_rep = self._deterministic_encoder(context_x, context_y, target_x)\n",
    "            representation = torch.cat([deterministic_rep, latent_rep],axis=-1)\n",
    "        else:\n",
    "            representation = latent_rep\n",
    "\n",
    "        dist, mu, sigma = self._decoder(representation, target_x)\n",
    "\n",
    "        # If we want to calculate the log_prob for training we will make use of the\n",
    "        # target_y. At test time the target_y is not available so we return None.\n",
    "        if target_y is not None:\n",
    "            log_p = dist.log_prob(target_y)\n",
    "            posterior = self._latent_encoder(target_x, target_y)\n",
    "            kl = torch.sum(kl_divergence(posterior, prior),-1,keepdim=True)\n",
    "            kl = kl.repeat([1, num_targets])\n",
    "            loss = -torch.mean(log_p -kl / num_targets.float())\n",
    "        else:\n",
    "            log_p = None\n",
    "            kl = None\n",
    "            loss = None\n",
    "\n",
    "        return mu, sigma, log_p, kl, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-Attention Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def uniform_attention(q, v):\n",
    "    \"\"\"Uniform attention. Equivalent to np.\n",
    "\n",
    "  Args:\n",
    "    q: queries. tensor of shape [B,m,d_k].\n",
    "    v: values. tensor of shape [B,n,d_v].\n",
    "    \n",
    "  Returns:\n",
    "    tensor of shape [B,m,d_v].\n",
    "  \"\"\"\n",
    "    total_points = q.shape[1]\n",
    "    rep = torch.mean(v, axis=1, keepdims=True)  # [B,1,d_v]\n",
    "    rep = rep.repeat([1, total_points, 1])\n",
    "    return rep\n",
    "\n",
    "\n",
    "def laplace_attention(q, k, v, scale, normalise):\n",
    "    \"\"\"Computes laplace exponential attention.\n",
    "\n",
    "  Args:\n",
    "    q: queries. tensor of shape [B,m,d_k].\n",
    "    k: keys. tensor of shape [B,n,d_k].\n",
    "    v: values. tensor of shape [B,n,d_v].\n",
    "    scale: float that scales the L1 distance.\n",
    "    normalise: Boolean that determines whether weights sum to 1.\n",
    "    \n",
    "  Returns:\n",
    "    tensor of shape [B,m,d_v].\n",
    "  \"\"\"\n",
    "    k = torch.unsqueeze(k, 1)  # [B,1,n,d_k]\n",
    "    q = torch.unsqueeze(q, 2)  # [B,m,1,d_k]\n",
    "    unnorm_weights = -torch.abs((k - q) / scale)  # [B,m,n,d_k]\n",
    "    unnorm_weights = torch.sum(unnorm_weights, axis=-1)  # [B,m,n]\n",
    "    if normalise:\n",
    "        weight_fn = F.softmax\n",
    "    else:\n",
    "        weight_fn = lambda x: 1 + torch.tanh(x)\n",
    "    weights = weight_fn(unnorm_weights)  # [B,m,n]\n",
    "    rep = torch.einsum('bik,bkj->bij', weights, v)  # [B,m,d_v]\n",
    "    return rep\n",
    "\n",
    "\n",
    "def dot_product_attention(q, k, v, normalise):\n",
    "    \"\"\"Computes dot product attention.\n",
    "\n",
    "  Args:\n",
    "    q: queries. tensor of  shape [B,m,d_k].\n",
    "    k: keys. tensor of shape [B,n,d_k].\n",
    "    v: values. tensor of shape [B,n,d_v].\n",
    "    normalise: Boolean that determines whether weights sum to 1.\n",
    "    \n",
    "  Returns:\n",
    "    tensor of shape [B,m,d_v].\n",
    "  \"\"\"\n",
    "    d_k = q.shape[-1]\n",
    "    scale = torch.sqrt(d_k.float())\n",
    "    unnorm_weights = torch.einsum('bjk,bik->bij', k, q) / scale  # [B,m,n]\n",
    "    if normalise:\n",
    "        weight_fn = F.softmax\n",
    "    else:\n",
    "        weight_fn = F.sigmoid\n",
    "    weights = weight_fn(unnorm_weights)  # [B,m,n]\n",
    "    rep = torch.einsum('bik,bkj->bij', weights, v)  # [B,m,d_v]\n",
    "    return rep\n",
    "\n",
    "\n",
    "def create_multihead_network(q_last_shape,k_last_shape,v_last_shape,num_heads=8):\n",
    "    \n",
    "    d_k = q_last_shape\n",
    "    d_v = v_last_shape\n",
    "    head_size = d_v / num_heads\n",
    "    conv_layers = []\n",
    "    for h in range(num_heads):\n",
    "        q_conv = nn.Conv1d(q_last_shape, head_size,1, bias = False, padding=0)\n",
    "        nn.init.normal(q_conv.weights, std =q_last_shape**(-.5))\n",
    "        k_conv = nn.Conv1d(k_last_shape, head_size,1, bias = False, padding=0)\n",
    "        nn.init.normal(k_conv.weights, std =q_last_shape**(-.5))\n",
    "        v_conv = nn.Conv1d(v_last_shape, head_size,1, bias = False, padding=0)\n",
    "        nn.init.normal(v_conv.weights, std =q_last_shape**(-.5))\n",
    "        rep_conv = nn.Conv1d(v_last_shape, v_last_shape,1, bias = False, padding=0)\n",
    "        nn.init.normal(rep_conv.weights, std =v_last_shape**(-.5))\n",
    "        \n",
    "        conv_layers.append([q_conv, k_conv,v_conv, rep_conv])\n",
    "        \n",
    "        return conv_layers\n",
    "\n",
    "def multihead_attention(conv_layers, q, k, v, num_heads=8):\n",
    "    \"\"\"Computes multi-head attention.\n",
    "\n",
    "  Args:\n",
    "    q: queries. tensor of  shape [B,m,d_k].\n",
    "    k: keys. tensor of shape [B,n,d_k].\n",
    "    v: values. tensor of shape [B,n,d_v].\n",
    "    num_heads: number of heads. Should divide d_v.\n",
    "    \n",
    "  Returns:\n",
    "    tensor of shape [B,m,d_v].\n",
    "  \"\"\"\n",
    "\n",
    "    rep = tf.constant(0.0)\n",
    "    \n",
    "    for h in range(num_heads):\n",
    "        o = dot_product_attention(\n",
    "            conv_layers[h][0](q),\n",
    "            conv_layers[h][1](k),\n",
    "            conv_layers[h][2](v), normalise=True)\n",
    "        rep += conv_layers[h][3](o)\n",
    "    return rep\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    \"\"\"The Attention module.\"\"\"\n",
    "    def __init__(self,rep,layer_sizes,att_type,scale=1.,normalise=True,num_heads=8):\n",
    "        \"\"\"Create attention module.\n",
    "\n",
    "        Takes in context inputs, target inputs and\n",
    "        representations of each context input/output pair\n",
    "        to output an aggregated representation of the context data.\n",
    "        Args:\n",
    "          rep: transformation to apply to contexts before computing attention. \n",
    "              One of: ['identity','mlp'].\n",
    "          output_sizes: list of number of hidden units per layer of mlp.\n",
    "              Used only if rep == 'mlp'.\n",
    "          att_type: type of attention. One of the following:\n",
    "              ['uniform','laplace','dot_product','multihead']\n",
    "          scale: scale of attention.\n",
    "          normalise: Boolean determining whether to:\n",
    "              1. apply softmax to weights so that they sum to 1 across context pts or\n",
    "              2. apply custom transformation to have weights in [0,1].\n",
    "          num_heads: number of heads for multihead.\n",
    "        \"\"\"\n",
    "        super(Attention, self).__init__()\n",
    "        self._rep = rep\n",
    "        self.Linears = nn.ModuleList([\n",
    "            nn.Linear(s1, s2)\n",
    "            for s1, s2 in zip(layer_sizes[:-1], layer_sizes[1:])\n",
    "        ])\n",
    "        self._type = att_type\n",
    "        self._scale = scale\n",
    "        self._normalise = normalise\n",
    "        if self._type == 'multihead':\n",
    "            self._num_heads = num_heads\n",
    "            self._conv_layers = create_multihead_network\n",
    "            \n",
    "    def forward(self, x1, x2, r):\n",
    "        \"\"\"Apply attention to create aggregated representation of r.\n",
    "\n",
    "        Args:\n",
    "          x1: tensor of shape [B,n1,d_x].\n",
    "          x2: tensor of shape [B,n2,d_x].\n",
    "          r: tensor of shape [B,n1,d].\n",
    "\n",
    "        Returns:\n",
    "          tensor of shape [B,n2,d]\n",
    "\n",
    "        Raises:\n",
    "          NameError: The argument for rep/type was invalid.\n",
    "        \"\"\"\n",
    "        if self._rep == 'identity':\n",
    "            k, q = (x1, x2)\n",
    "        elif self._rep == 'mlp':\n",
    "            # Pass through MLP\n",
    "            k = forward_pass(x1, self.Linears)\n",
    "            q = forward_pass(x2, self.Linears)\n",
    "        else:\n",
    "            raise NameError(\"'rep' not among ['identity','mlp']\")\n",
    "\n",
    "        if self._type == 'uniform':\n",
    "            rep = uniform_attention(q, r)\n",
    "        elif self._type == 'laplace':\n",
    "            rep = laplace_attention(q, k, r, self._scale, self._normalise)\n",
    "        elif self._type == 'dot_product':\n",
    "            rep = dot_product_attention(q, k, r, self._normalise)\n",
    "        elif self._type == 'multihead':\n",
    "            rep = multihead_attention(q, k, r, self._num_heads)\n",
    "        else:\n",
    "            raise NameError(\n",
    "                (\"'att_type' not among ['uniform','laplace','dot_product'\"\n",
    "                 \",'multihead']\"))\n",
    "\n",
    "        return rep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-03T12:53:31.813220Z",
     "start_time": "2020-08-03T12:53:31.807000Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-04T07:48:52.544759Z",
     "start_time": "2020-08-04T07:48:52.530472Z"
    }
   },
   "outputs": [],
   "source": [
    "c = nn.Conv1d(10,11,1 bias = False, padding=0)\n",
    "\n",
    "c(torch.randn(1,10,6)).shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepmind",
   "language": "python",
   "name": "deepmind"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
